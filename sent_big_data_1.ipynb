{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e03710-788f-43ac-8849-ee960db56e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requeriments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac2e48ce-9748-4cf3-9256-57150dd7ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Params is: Ok\n",
      "The Data Model is: Ok\n",
      "The sentiment export to list is: Ok\n",
      "117\n",
      "gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Output_batch/prediction-bss_sss_onsite-2023-02-10T06:04:21.473696Z/predictions_00001.jsonl\n",
      "First state is : OK\n",
      "Second state is : OK\n",
      "gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Output_batch/prediction-bss_sss_onsite-2023-02-10T06:04:21.473696Z/predictions_00002.jsonl\n",
      "First state is : OK\n",
      "Second state is : OK\n",
      "Data_frame is complete: OK\n",
      "The file is  : OK\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from typing import Sequence, Union\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" This function performs a batch prediction job using the specified AI Platform model,\n",
    "    source and destination.Returns: output_files (List[str]): The names of the files in \n",
    "    the GCS destination location that contain the prediction results. \"\"\"\n",
    "\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: Union[str, Sequence[str]],\n",
    "    gcs_destination: str,\n",
    "    sync: bool = True,\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    # Initialize the AI Platform client for the given project and location\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Get the AI Platform model to use for prediction\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    # Submit the batch prediction job using the model, source, and destination\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=sync,\n",
    "    )\n",
    "    \n",
    "    # Get the names of the prediction results files in the GCS destination location\n",
    "    client = storage.Client(project=project)\n",
    "    bucket = client.bucket(gcs_destination.split(\"/\")[2])\n",
    "    blobs = bucket.list_blobs(prefix=gcs_destination.split(\"/\", 3)[-1])\n",
    "    output_files = [blob.name for blob in blobs]\n",
    "    \n",
    "    return output_files\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Param:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # path to the input file\n",
    "        self.file_input = 'gs://data_sentiment_v5/SSS/sss_onsite/on_site_2023/SSS DB Power BI - Sentimiento OnSite  1-5 Febrero 2023.xls\n",
    "        # project ID \n",
    "        self.project_id = \"284757810904\"\n",
    "        # model ID for off-site sentiment analysis\n",
    "        self.model_id_off_site =\"5835259941211865088\"\n",
    "        # model ID for on-site sentiment analysis\n",
    "        self.model_id_on_site = \"3580645377759510528\"\n",
    "        # input URI for batch predictions\n",
    "        self.input_uri = \"gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Main_Control_Batch/Prediction_Main_Batch.jsonl\"\n",
    "        # output URI for batch predictions\n",
    "        self.output_uri = \"gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Output_batch/\"\n",
    "        # location for cloud services\n",
    "        self.location=\"us-central1\"\n",
    "        # display name for the batch prediction job\n",
    "        self.job_display_name = \"new_job\"\n",
    "        # path for processing batch data\n",
    "        self.Processing_Batch = 'gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Prosecing_Masive_Data/'\n",
    "        # path for main control batch data\n",
    "        self.Path_Control = 'gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Main_Control_Batch/'\n",
    "        # initial string for constructing the content URI\n",
    "        self.complement_ini = \"{\\'content\\' :\\'gs://batch_predictions_sentiment/Prediction_Masive_Sentiment/Prosecing_Masive_Data/\"\n",
    "        # end string for constructing the content URI\n",
    "        self.complement_end = \".txt\\', \\'mimeType\\': \\'text/plain\\'}\"\n",
    "                               \n",
    "\n",
    "\n",
    "class Input_Data:\n",
    "    def __init__(self, Route):\n",
    "        #Route (str): The path to the input file.\n",
    "        self.Route = Route\n",
    "    \n",
    "    def Data_Sent_Filt(self):\n",
    "        \n",
    "    \"\"\"Reads the input file and returns a DataFrame with the \n",
    "       columns \"Id_Model\" and \"Message\".Returns:Data (pandas.DataFrame):\n",
    "       A DataFrame containing the columns \"Id_Model\" and \"Message\".\"\"\"\n",
    "        \n",
    "        df = pd.read_excel(self.Route)\n",
    "        Data = df.loc[:,['Id_Model','Message']]\n",
    "        return Data\n",
    "    \n",
    "    def Data_Sent_All(self):\n",
    "        \n",
    "        df = pd.read_excel(self.Route)\n",
    "        return df\n",
    "\n",
    "    \n",
    "\"\"\"This object we will found the code that is in charge to convert\n",
    "        and clean the data from the buckets\"\"\"\n",
    "\n",
    "    \n",
    "class Data_Frame:\n",
    "    def __init__(self, Data):\n",
    "        self.Data = Data\n",
    "    \n",
    "    def Transform(self):\n",
    "        \n",
    "        All_Clean = self.Data\n",
    "        Sentimentnot = All_Clean[All_Clean.Message.notnull()]\n",
    "        return Sentimentnot\n",
    "    \n",
    "    \n",
    "\"\"\"This part we will found the code will  convert the sentiment \n",
    "    data in a generator to meke the reuest to the model \"\"\"    \n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, Sentiment,Path):\n",
    "        \n",
    "        self.Sentiment = Sentiment\n",
    "        self.Path = Path\n",
    "        \n",
    "    def Iterator(self):\n",
    "        \n",
    "        df = self.Sentiment\n",
    "        \n",
    "        Sentiment_df = df['Message']\n",
    "        \n",
    "        General_list = Sentiment_df.to_numpy().tolist()\n",
    "        \n",
    "        New_df = pd.DataFrame (General_list, columns = ['Sentiment'])\n",
    "        \n",
    "        for i in range(len(New_df)):\n",
    "            \n",
    "            df_fn = (New_df.loc[i, 'Sentiment'])\n",
    "            \n",
    "            new_list = [df_fn]\n",
    "            \n",
    "            df_second = pd.DataFrame(new_list, columns = [''])\n",
    "            \n",
    "            df_second.to_csv(f\"{self.Path}{i}.txt\",index=False,header=False)\n",
    "        \n",
    "        return New_df \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"With this code we generate paths to GET BATCH PREDICTION\"\"\"    \n",
    "class Generate_path:\n",
    "    def __init__(self, Senti_df,Path):\n",
    "        \n",
    "        self.Senti_df = Senti_df\n",
    "        self.Path = Path\n",
    "         \n",
    "    def paths_jsl(self, a, b):\n",
    "        \n",
    "        \n",
    "        df = self.Senti_df\n",
    "\n",
    "        print(range(len(df)))\n",
    "        \n",
    "        new_list  = df.to_numpy().tolist()\n",
    "        \n",
    "        list_names = range(len(new_list))\n",
    "        list_paths = [] \n",
    "        \n",
    "        for i in list_names:\n",
    "            c = str(i)\n",
    "            Paths = a + c + b\n",
    "            list_paths.append(Paths)\n",
    "        \n",
    "        New_df = pd.DataFrame (list_paths, columns = ['Paths_off_Sentiment'])\n",
    "        New_df.to_csv(f'{self.Path}Prediction_Main_Batch.jsonl',index=False,header=False,quotechar=' ')\n",
    "        return New_df\n",
    "        \n",
    "        \n",
    "\"\"\"With this code we generate a list \"\"\"\n",
    "class generate_list:\n",
    "        \n",
    "    def create_list(self,files_user):\n",
    "    \n",
    "        my_list = []\n",
    "    \n",
    "        for i in range(1,files_user+1):\n",
    "            my_list.append(i)\n",
    "    \n",
    "        formatter = \"{:02d}\".format\n",
    "        my_list = list(map(formatter, my_list))\n",
    "        \n",
    "        return my_list\n",
    "            \n",
    "\n",
    "\n",
    "\"\"\"With this object we are gonna see the code to load the data from buckets,\n",
    "        in this case the Data we will load is the sentiment\"\"\"\n",
    "\n",
    "class Input_Result:\n",
    "    def __init__(self, Result):\n",
    "\n",
    "        self.Result = Result\n",
    "    \n",
    "    def Data_Out(self):\n",
    "        columts = ['1', '2', '3','4','5','Order','Sentiment']\n",
    "        Data = pd.read_csv(self.Result, names=columts,header=None, sep='/')\n",
    "        return Data\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"Whit this object we will count the files that the job prediction has done before\"\"\"\n",
    "    \n",
    "    \n",
    "def generator_0 (count_files):\n",
    "    \n",
    "    #Read  batch prediction\n",
    "    \n",
    "    for i in count_files:\n",
    "        \n",
    "        Results = Input_Result(i)\n",
    "        Data_Result = Results.Data_Out()\n",
    "        \n",
    "    \n",
    "        #organize and separate\n",
    "        Data_Recl = Data_Result.loc[:,['Order','Sentiment']]\n",
    "        \n",
    "        yield Data_Recl\n",
    "    \n",
    "    \n",
    "\"\"\"Whit this object we will extract the number of each sentiment\"\"\"\n",
    "    \n",
    "def generator_1(Data_Recl):\n",
    "    \n",
    "    df_num = pd.DataFrame()\n",
    "    df_num = df_num.fillna(0)\n",
    "    for i in Data_Recl.columns:\n",
    "        \n",
    "        df_num[i]=Data_Recl [i].str.extract('(\\d+(?:\\.\\d+)?)')\n",
    "    \n",
    "    yield df_num\n",
    "\n",
    "\n",
    "    \n",
    "def cycle_1(primer_g,a,row_max,df_final):\n",
    "    \n",
    "    while a != row_max:\n",
    "        \n",
    "        #First state of sentiment\n",
    "        \n",
    "        state_gene_1 = next(primer_g)\n",
    "        #print(\"First state is : OK\")\n",
    "    \n",
    "        #Second state of sentiment\n",
    "        segundo_g = generator_1(state_gene_1)\n",
    "        state_gene_2 = next(segundo_g)\n",
    "        #print(\"Second state is : OK\")\n",
    "    \n",
    "        #merge the data frame\n",
    "        df_final =pd.concat([df_final,state_gene_2],ignore_index=True)\n",
    "        a = int(len(df_final))\n",
    "        \n",
    "    return df_final\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\"\"\"This is the main code to start the program in this part we call the different objects\"\"\"\n",
    "\n",
    "def run():\n",
    "    \n",
    "    #Call Params to find read data\n",
    "    Params = Param()\n",
    "    Route_m = Params.file_input\n",
    "    print(\"The Params is: Ok\")\n",
    "    \n",
    "    \n",
    "    #Extract the Data Filt\n",
    "    All_Data = Input_Data(Route_m)\n",
    "    Data_1 = All_Data.Data_Sent_Filt()\n",
    "    print(\"The Data is: Ok\")\n",
    "    \n",
    "    \n",
    "    #Extract the Sentiment \n",
    "    Extract = Data_Frame(Data_1)\n",
    "    Data_Sentiment = Extract.Transform()\n",
    "    print(\"The sentiment export to list is: Ok\")\n",
    "    \n",
    "    \n",
    "    #Generate Data files txt\n",
    "    Variable_S = Generator(Data_Sentiment, Params.Processing_Batch)\n",
    "    Sentiments = Variable_S.Iterator()\n",
    "    print (\"The sentiment already exported to txt : Ok \")\n",
    " \n",
    "    \n",
    "    #Generate list with paths of sentiments \n",
    "    list_sent = Generate_path(Data_Sentiment, Params.Path_Control)\n",
    "    jsonl = list_sent.paths_jsl(Params.complement_ini, Params.complement_end)\n",
    "    print (\"The Paths  prediction exported : OK \" )\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Batch prediction of sentiments\n",
    "\n",
    "    output_files= batch_prediction_job(Params.project_id, \n",
    "                         Params.location,                     \n",
    "                         #change depend of the model offsite - onsite\n",
    "                         Params.model_id_on_site, \n",
    "                         Params.job_display_name,\n",
    "                         Params.input_uri,\n",
    "                         Params.output_uri)\n",
    "    \n",
    "    print(\"Prediction Batch: ok\")\n",
    "    \n",
    "    \n",
    "    #fix files output batch prediction\n",
    "    output_files.pop(0)\n",
    "    string = \"gs://batch_predictions_sentiment/\"\n",
    "    new_list = list(map(lambda x: string + str(x), output_files)) \n",
    "    print('files of batch is : ok')\n",
    "    \n",
    "    \n",
    "    #Extract the  ALL Data \n",
    "    Data_2 = All_Data.Data_Sent_All()\n",
    "    print(\"The Data Model is: Ok\")\n",
    "    \n",
    "    \n",
    "    #Extract the Sentiment \n",
    "    Extract = Data_Frame(Data_2)\n",
    "    Data_Sentiment = Extract.Transform()\n",
    "    print(\"The sentiment export to list is: Ok\")\n",
    "    \n",
    "    #concat and organize the data of sentiment\n",
    "    df_final = pd.DataFrame()\n",
    "    primer_g = generator_0(new_list)\n",
    "    a = 0\n",
    "    row_max = (int(len(Data_Sentiment)))\n",
    "    print(\"number of registres is: \" + str(row_max))\n",
    "    df_final = cycle_1(primer_g,a,row_max,df_final)\n",
    "    \n",
    "    #Order and index of dataframe\n",
    "    print(\"Data_frame is complete: OK\")\n",
    "    df_order = df_final.sort_values('Order',ascending=True) \n",
    "    df_order['Order'] = df_order['Order'].astype(int)\n",
    "    df_order = df_order.set_index('Order')\n",
    "    key_df  = pd.merge(Data_Sentiment, df_order, left_index=True, right_index=True)\n",
    "    \n",
    "    #Rename columns\n",
    "    key_df.rename(columns = {'Sentiment_y':'Sentiment_M', 'Sentiment_x':'Sentiment_H'}, inplace = True)\n",
    "    \n",
    "    #Export file\n",
    "    key_df.to_excel(\"data/sentiment.xlsx\",index=False)\n",
    "    print(\"The file is  : OK\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec2b30-540c-45b9-9512-7e283d967609",
   "metadata": {},
   "source": [
    "Este código es un script en Python que permite realizar predicciones en masa de un modelo de Google Cloud AI Platform utilizando el API de batch prediction. La implementación se divide en diferentes clases que describen los diferentes componentes del código.\n",
    "\n",
    "La clase batch_prediction_job es la función principal del código que realiza la predicción en masa. Toma como parámetros el proyecto y la ubicación en Google Cloud, el nombre del recurso del modelo, el nombre del trabajo de predicción en masa, la fuente de datos y la ubicación de destino de los resultados.\n",
    "\n",
    "La clase Param define los parámetros que se utilizan en el código, incluido el nombre de proyecto, los identificadores de modelo, la ubicación, la ruta de entrada y la ruta de salida, así como ciertos parámetros relacionados con la manipulación de datos masivos.\n",
    "\n",
    "La clase Input_Data se utiliza para cargar los datos de los buckets de Google Cloud Storage y puede acceder a los datos de sentimiento en un dataframe utilizando los métodos Data_Sent_Filt y Data_Sent_All.\n",
    "\n",
    "La clase Data_Frame se utiliza para limpiar y transformar los datos de sentimiento antes de ser enviados al modelo.\n",
    "\n",
    "La clase Generator convierte los datos de sentimiento en un generador para su uso en la predicción en masa.\n",
    "\n",
    "En resumen, este código permite la carga y manipulación de datos de sentimiento de un bucket de Google Cloud Storage y su posterior predicción en masa utilizando un modelo de Google Cloud AI Platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
